---
title: "Missing Data"
subtitle: "Princeton University"
author: "Suyog Chandramouli (adapted from slides by Jason Geller and Richard McElreath)"
date: 'Updated:`r Sys.Date()`'
footer: "PSY 504: Advaced Statistics"
format: 
  revealjs:
    theme: white
    css: slide-style.css
    multiplex: true
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    fontsize: "25pt"
    highlight-style: github-dark
execute:
  freeze: auto
  echo: true
  message: false
  warning: false
  fig-align: center
  fig-width: 16
  fig-height: 10
  editor_options: 
  chunk_output_type: inline
  code-overflow: wrap
  html:
    code-fold: true
    code-tools: true
bibliography: references.bib
---

## Today

::::: columns
::: {.column width="50%"}
-   MCAR, MAR, NMAR

-   Screening data for missingness

-   Diagnosing missing data mechanisms in R

-   Missing data methods in R

    -   Listwise deletion
    -   Casewise deletion
    -   Nonconditional and conditional imputation
    -   Multiple imputation
    -   Maximum likelihood

-   Reporting
:::

::: {.column width="50%"}
![Richard McElreath](images/terminator.png){fig-align="center" width="296"}
:::
:::::

## Discuss

Have you ever had to deal with missing data in your studies or in your dataset?

-   What caused it?

-   How did you deal with it?

## Packages

Install the **mice** package

```{r, eval = F}
install.packages("mice")
```

Load these packages:

```{r, warning = F, message = F}
library(tidyverse) 
library(easystats)
library(knitr)
library(broom) #tidy statistics
library(ggmice) #graph missing data
library(mice) # dealing and visualizing missing data
library(naniar) # missing data + visualization
library(finalfit)# missing data visualization

```

Here is the link to the .qmd document to follow along: <https://github.com/jgeller112/PSY504-Advanced-Stats-S24/blob/main/slides/03-Missing_Data/03-Missing_Data.qmd>.

## Missing data mechanisms

::::: columns
::: {.column width="50%"}
-   Most of modern missing data theory comes from the work of statistician Donald B. Rubin

-   Rubin proposed we can divide an entire data set $Y$ into two components:

    -   $Y_\text{obs}$ the observed values in the data set

    -   $Y_\text{mis}$ the missing values in the data set

$$Y = Y_\text{obs} + Y_\text{mis}$$
:::

::: {.column width="50%"}
<br>

<br>

![](images/Screen%20Shot%202024-01-04%20at%207.46.41%20AM.png){fig-align="center"}
:::
:::::

<!-- ## Missing data mechanisms: The framework -->

<!-- Missing data can occur through different processes -->
<!-- Missingness may relate systematically to parts of our data -->
<!-- These mechanisms serve as statistical assumptions -->
<!-- They guide our analytical approach -->



## Missing data mechanisms

-   Missing data mechanisms (processes) describe different ways in which the data relate to nonresponse

-   Missingness may be completely random or systematically related to different parts of the data

-   ***Mechanisms function as statistical assumptions***

-   this framework remains foundational

-   is the theoretical/quantitative backbone for understanding and accounting for missing data

<!-- ## Dog & Homework example (3 mechanisms) -->

<!-- ![](images/all_0.png){fig-align="center"} -->

## Dog & Homework example (3 mechanisms)

![](images/DogHomework_DAG.png){fig-align="center"}


## Missing completely at random (MCAR)

::::: columns

::: {.column width="50%"}
- Dog eats homework randomly

- No relationship between missingness and any values

- Results in loss of precision, but usually no bias

- Simple approaches like complete-case analysis valid

- MCAR is purely random missingness
:::

::: {.column width="50%"}
![](images/Mcelreath/Lecture_18_Page_09.jpeg){fig-align="center"}
:::

:::::

## Missing completely at random (MCAR)

::::: columns

::: {.column width="50%"}
- Dog eats homework randomly

- No relationship between missingness and any values

- Results in loss of precision, but usually no bias

- Simple approaches like complete-case analysis valid

- MCAR is purely random missingness
:::

::: {.column width="50%"}
![](images/mcar.png){fig-align="center"}
:::
:::::


## 
![](images/Mcelreath/Lecture_18_Page_11.png){fig-align="center"}


## 
![](images/Mcelreath/Lecture_18_Page_12.png){fig-align="center"}


## Conditionally missing at random (CMAR)

::::: columns
::: {.column width="50%"}
Missingness depends on observed data

- Dog eats conditional on cause of homework

Can produce valid inference if we condition correctly

:::
::: {.column width="50%"}
![](images/Mcelreath/Lecture_18_Page_14.jpeg){fig-align="center"}
:::
:::::


## 
![](images/Mcelreath/Lecture_18_Page_16.png){fig-align="center"}


## 
![](images/Mcelreath/Lecture_18_Page_17.png){fig-align="center"}

## Conditionally missing at random (CMAR)

::::: columns
::: {.column width="50%"}
Missingness depends on observed data

- Dog eats conditional on cause of homework

Can produce valid inference if we condition correctly

:::

::: {.column width="50%"}
![](images/cmar.png){fig-align="center"}
:::
:::::

## CMAR: When things get complex
::::: columns
::: {.column width="50%"}

- Sometimes relationships aren't simple linear patterns

- Poor modeling of MAR can lead to bias

- Non-linear relationships require careful handling

- Multiple imputation and maximum likelihood methods work well
:::
:::::

## Not missing at random (NMAR)

::::: columns
::: {.column width="50%"}


Missingness depends on unobserved values

- Dog eats conditional on homework itself

- Usually produces bias in standard analyses
- Requires specialized approaches

:::

::: {.column width="50%"}
![](images/Mcelreath/Lecture_18_Page_19.jpeg){fig-align="center"}
:::
:::::

## Not missing at random (NMAR)

::::: columns
::: {.column width="50%"}


Missingness depends on unobserved values

- Dog eats conditional on homework itself

- Usually produces bias in standard analyses
- Requires specialized approaches

:::

::: {.column width="50%"}
![](images/nmar.png){fig-align="center"}
:::
:::::

## 
![](images/Mcelreath/Lecture_18_Page_21.png){fig-align="center"}



# Workflow for missing data
  - Identify if there is missing data

  - Diagnose the missingness generating process

  - Account for missingness appropriately 

      - deletion (not recommended)

      - guess and fill-in the missing values, aka, imputation

      - Maximum likelihood
      
      - Bayesian methods:
      
        - no imputation step
        
        - we model observed and missing data jointly and 'integrate over' missing values
    



# Data

## Chronic pain example

-   Enders (2023)

    -   Study (*N* = 275) investigating psychological correlates of chronic pain

        -   Depression (`depress`)
        -   Perceived control (`control`)

-   Perceived control over pain is complete, depression scores are missing

    ::: callout-note
    I manipulated the dataset so missingness is related to control over pain (i.e., low control is related to missingness on depression scores)
    :::

## Chronic pain example

::::: columns
::: {.column width="50%"}
```{r}

dat <- read.table("https://raw.githubusercontent.com/jgeller112/PSY504-Advanced-Stats-S24/main/slides/03-Missing_Data/APA%20Missing%20Data%20Training/Analysis%20Examples/pain.dat", na.strings = "999")

names(dat) <- c("id", "txgrp", "male", "age", "edugroup", "workhrs", "exercise", "paingrps", 
                "pain", "anxiety", "stress", "control", "depress", "interfere", "disability",
                paste0("dep", seq(1:7)), paste0("int", seq(1:6)), paste0("dis", seq(1:6)))

dat <- dat  %>%
  select("id", "age", "control",  "depress", "stress") %>%
  mutate(depress=ifelse(depress==999, NA, depress)) %>%
    mutate(r_mar_low = ifelse(control < 15.51, 1, 0)) %>% 
  mutate(depress = ifelse(r_mar_low == 1, NA, depress)) %>%
  select(-r_mar_low)

```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| 
dat <- dat  %>%
  select("id", "age", "control",  "depress", "stress") %>%
  mutate(depress=ifelse(depress==999, NA, depress)) %>%
    mutate(r_mar_low = ifelse(control < 15.51, 1, 0)) %>% 
  mutate(depress = ifelse(r_mar_low == 1, NA, depress)) %>%
  select(-r_mar_low)

dat %>%
  kable()

```
:::
:::::

## Exploratory data analysis (EDA)

-   Look at your data
-   Need to identify missing data!
    -   Explore data using descriptive statistics and figures

        -   What variables have missing data? How much is missing in total? By variable?

## EDA

```{r}
#| fig-align: "center"
library(naniar)
vis_miss(dat)
```

## EDA

```{r}

library(skimr)
skimr::skim(dat) %>% 
  kable()

```

## Missing patterns

::::: columns
::: {.column width="50%"}
-   A missing data pattern matrix is the structure of observed and missing values in the dataset
    -   Where the data is missing

```{r}
#md.pattern(dat)
#ggmice
dat %>%
 # create missing data pattern plot
plot_pattern()

```
:::

::: {.column width="50%"}
<br>

<br>

<br>

```{r}
#| echo: false
#| fig-align: "center"
knitr::include_graphics("images/missing_patterns.png")
```
:::
:::::

## Missing patterns

::::: columns
::: {.column width="50%"}
-   Univariate: one variable with missing data

-   Monotone: patterns in the data can be arranged

    -   Associated with a longitudinal studies where members drop out and never return

-   Non-monotone: missingness of one variable does not affect the missingness of any other variables

    -   Look for islands of missingness
:::

::: {.column width="50%"}
```{r}
#| fig-align: "center"
#| echo: false
#| 

knitr::include_graphics("images/new_patterns.webp")
```
:::
:::::

## Is it MCAR or MAR?

![](images/marcar.png){fig-align="center"}

## Is it MCAR or MAR?

-   Can make a case for MCAR

    -   Little's test

        -   $\chi^2$

            -   Sig = not MCAR

            -   Not sig = MCAR

```{r}
library(naniar)

mcar_test(dat) %>% kable()
```

-   Not used much anymore!

## Is it MCAR or MAR?

```{r, echo=TRUE, fig.align='center', out.width="100%"}
library(finalfit)

explanatory = c("control", "age")
dependent = "depress" 

misspairs <- dat %>%
  missing_pairs(explanatory, dependent)
   # mice function visualize data
```

```{r, echo=FALSE, fig.align='center', out.width="100%"}
misspairs
```

## Is it MCAR or MAR?

-   Our job is to find out if our data is MAR

    -   Create a dummy coded variable for missing variable where 1 = score missing and 0 = score not missing on missing variable

        -   If these variables are related to other variables in dataset

            -   MAR

```{r}
pain_r <- dat %>%#can also use case_when #if missing 1 else 
  mutate(depress_1 = ifelse(is.na(depress), 1, 0))
pain_r %>% head() %>% kable()

```

## Testing

-   lm, *t*-test, or glm

```{r}
model <- lm(control~depress_1,data=pain_r)

tidy(model) %>%
  kable()
```

-   It looks like missing on depression is related to control!

# Methods for dealing with MCAR

## Listwise deletion

::::: columns
::: {.column width="50%"}
```{r}
dat %>%
  kable()
```
:::

::: {.column width="50%"}
```{r}

dat  %>%
  drop_na() %>%
  kable()

```
:::
:::::

## Listwise deletion: pros and cons

-   Pros:

    -   Produces the correct parameter estimates if missingness is MCAR

        -   If not, biased

-   Cons:

    -   Can result in a lot of data loss

## Casewise (pairwise) deletion

-   In each comparison, delete only observations if the missing data is relevant to this comparison

```{r, echo=TRUE}
#create data frame
dat %>%
  kable()
```

## Casewise deletion: pros and cons

Pros:

-   Avoids data loss

-   Non-biased

    -   Only for MCAR

Cons:

-   But, results not completely consistent or comparable--based on different observations

# Methods for MAR

## Unconditional (mean) imputation - Bad

-   Replace missing values with the mean of the observed values

    -   Reduces variance

        -   Increases Type 1 error rate

::::: columns
::: {.column width="50%"}
```{r}

d <- c(5, 8, 3, NA, NA)

#calc mean remove NAs
d_mean <- mean(d, na.rm = TRUE)

d_mean_imp <- ifelse(is.na(d), d_mean, d) # add mean

d_mean_imp
```
:::

::: {.column width="50%"}
```{r}

sd(d, na.rm=TRUE) # sd of org dataset
sd(d_mean_imp) # sd of mean impute dataset

#NANIRE Package 
#impute_mean(d) %>% head()
# Do this in Mice
#complete(mice(data, m=1, method="mean"))
```
:::
:::::

## Conditional imputation (regression)

::::: columns
::: {.column width="50%"}
-   Run a regression using the complete data to replace the missing value

```{r}
#| eval: false
#| 

lm(depress~control) # on complete data

imp.regress <- mice(dat, method="norm.predict", m=1, maxit=1) # norm.predict performs regression impute


```

-   All the other related variables in the data set are used to predict the values of the variable with missing data

-   Missing scores have the predicted values provided to replace them
:::

::: {.column width="50%"}
![](images/reg_imp.png){fig-align="center"}
:::
:::::

## Stochastic Regression

- This is regression with added randomness to predicted values

- Why?
  - Because predicted values fall perfectly on regression line - too "neat" compared to real data.
    - This underestimates the variance in regression data
  - Results in more realistic standard errors
  
  

```{r}
#| eval: false
#| 
mice(data, m=1, method="norm.nob") # perfrom stoch regression 

```

![](images/stoch_reg.png){fig-align="center"}

# MI

## Multiple Imputation

-   Instead of using one value as a true value (which ignores uncertainty and variance), we use multiple values

-   Basically doing conditional imputation several times

    -   Several steps





## Multiple Imputation
1.  We make several multiply imputed data sets with the `mice()` function

![](images/Screen%20Shot%202024-01-04%20at%205.05.58%20PM.png){fig-align="center"}

## Multiple Imputation

2.  We fit our model of choice to each version of the data with the `with()` function

![](images/Screen%20Shot%202024-01-04%20at%205.06.45%20PM.png){fig-align="center"}

## Multiple Imputation

3.  We then pool (i.e., combine) the results with the `pool()` function

![](images/Screen%20Shot%202024-01-04%20at%205.07.17%20PM.png){fig-align="center"}

## Multiple Imputation

![](images/mice_comp.PNG){fig-align="center" width="666"}

![](){fig-align="center"}

## Multiple Imputation

-   Philosophically analogous to bootstrap. Both:

    - Generate multiple datasets
    
    - Analyze each dataset separately
    
    - Pool results for final estimates
    
-   Boostrap deals with sampling uncertainty, MI with missing data uncertainty

-   Note: these can be combined into bootstrap multiple imputation!


## 1. Impute with `Mice`

```{r}
m=5
# impute several data sets
imp <- mice(dat, m = m, seed = 24415, method="pmm", print = FALSE)

```

-   What is `imp`?

```{r}
str(imp, max.level = 1)
```

## 1. Impute with `Mice`

-   What is `imp` within `imp`?

```{r}
# uses all data to imputet even ID so change accordingly
str(imp$imp, max.level = 1)

#get the imputed values for that var
head(imp$imp$depress)

#get the imputed datasets out
#complete(imp, "all")

```

## PMM

::::: columns
::: {.column width="50%"}
-   Predictive mean matching

    -   For each missing value, a regression model is fitted using the observed (complete) data, where the variable with missing data is the outcome and other variables are predictors
    -   For a record with a missing value, the fitted model predicts a mean based on the available data
:::

::: {.column width="50%"}
-   PMM identifies a set of "donors" from the observed data. These donors are the cases whose predicted means are closest to the predicted mean of the case with the missing value

![](images/PMM.PNG){width="435" height="319"}
:::
:::::

## 2. Model with `Mice`

-   We'll fit a simple statistical model

```{r}
#fit the model to each set of imputaed data

fit <- with(data = imp, expr = lm(depress ~ control))

summary(fit) %>%
  kable()
```

## 3. `Mice` Pool Results

```{r}
#combine the results
result <- pool(fit)

model_parameters(result) %>%
  kable()
```

## 3. `Mice` Pool Results

-   `emmeans` does not play nicely with `mice` objects

-   `marginaleffects`

    -   Can be used to perform hypothesis tests on coefficients

```{r}
library(marginaleffects)

mfx_mice <- avg_slopes(fit) # avg_comparions for categorical vars

mfx_mice %>% kable()

```

## Plot Imputations

-   Make sure they look similar to real data

```{r, fig.align='center', out.width="60%"}
# create stripplot 
ggmice(imp, ggplot2::aes(x = .imp, y = depress)) +
  ggplot2::geom_jitter() + 
    labs(x = "Imputation number")
```

# Maximum likelihood (ML)

## ML

-   Determines the most probable settings (parameter estimates) for a statistical model by making the model's predicted outcomes as close as possible to the observed data

-   Each observation’s contribution to estimation is restricted to the subset of parameters for which there is data 

-   Estimation uses incomplete data, no imputation performed 

## ML

Implicit imputation

-   Each participant contributes their observed data

-   Data are not filled in, but the multivariate normal distribution acts like an imputation machine

-   The location of the observed data implies the probable position of the unseen data, and estimates are adjusted accordingly 

![](images/Screen%20Shot%202024-01-04%20at%205.30.44%20PM.png){fig-align="center" width="538"}

## Chronic pain illustration

-   Participants with low perceived control are more likely to have missing depression scores (conditionally MAR)

-   The true means are both 20

![](images/Screen%20Shot%202024-01-04%20at%205.31.57%20PM.png){fig-align="center" width="598"}

## Deleting incomplete information

::::: columns
::: {.column width="50%"}
-   Deleting cases with missing depression scores gives a non-representative sample

-   The perceived control mean is too high (Mpc = 23.1), and the depression mean is too low (Mdep = 17.2) 
:::

::: {.column width="50%"}
![](images/Screen%20Shot%202024-01-04%20at%205.34.18%20PM.png){fig-align="center" width="607"}
:::
:::::

## Partial data

-   Incorporating the partial data gives a complete set of perceived control scores

-   The partial data records primarily have low perceived control scores 

![](images/Screen%20Shot%202024-01-04%20at%205.35.03%20PM.png){fig-align="center" width="605"}

## Adjusting perceived control

-   Adding low perceived control scores increases the variable's variability

-   The perceived control mean receives a downward adjustment to accommodate the influx of low scores 

![](images/Screen%20Shot%202024-01-04%20at%205.36.26%20PM.png){fig-align="center"}

## Implicit Imputation

-   Maximum likelihood assumes multivariate normality

-   In a normal distribution with a negative correlation, low perceived control scores should pair with high depression 

![](images/Screen%20Shot%202024-01-04%20at%205.37.37%20PM.png){fig-align="center"}

## Adjusting Depression Distribution

-   Maximum likelihood intuits the presence of the elevated but unseen depression scores

-   The mean and variance of depression increase to accommodate observed perceived control scores at the low end 

![](images/ML_unbiased.png){fig-align="center" width="559"}

## ML: Cons

-   Generally limited to normal data, options for mixed metrics are less common

-   Normal-theory methods are biased with interactions and non-linear terms

-   MLM software usually discards observations with missing predictors 

## Distinguish between NMAR and MAR

-   Pray you don't have to 😂

-   It's complicated

    -   Not many good techniques

-   NMAR into MAR

    -   Try and track down the missing data

    -   Auxiliary variables

    -   Collect more data for explaining missingness

## Distinguish between NMAR and MAR

![](images/reddit.jpg){fig-align="center"}


## Bayesian Methods

::::: columns
::: {.column width="50%"}
**Without Missing Data**

Model specification:

- y_i ~ Normal(α + βx_i, σ²)  # Data model

- α ~ Normal(0, 10)           # Prior for intercept

- β ~ Normal(0, 1)            # Prior for slope

- σ ~ HalfCauchy(0, 5)        # Prior for variance

:::

::: {.column width="50%"}
**With Missing Data**

Model specification:

- y_i ~ Normal(α + βx_i, σ²)            # Data model

- x_i ~ Normal(μx, τ²) for missing x_i   # Model for missing values

- α ~ Normal(0, 10)                      # Prior for intercept

- β ~ Normal(0, 1)                       # Prior for slope

- σ ~ HalfCauchy(0, 5)                   # Prior for variance

- μx ~ Normal(0, 10)                     # Prior for x mean

- τ ~ HalfCauchy(0, 5)                   # Prior for x variance

:::
::::

Estimation? MCMC sampling gives posterior distributions for parameters

## Reporting Missing Data

-   Template from [Stepf van Buuren](https://stefvanbuuren.name/fimd/sec-reporting.html)

::: callout-tip
The *percentage of missing values* across the nine variables varied between *0 and 34%*. In total *1601 out of 3801 records (42%)* were incomplete. Many girls had no score because the nurse felt that the measurement was "unnecessary," or because the girl did not give permission. Older girls had many more missing data. We used *multiple imputation* to create and analyze *40 multiply imputed datasets*. Methodologists currently regard multiple imputation as a state-of-the-art technique because it improves accuracy and statistical power relative to other missing data techniques. *Incomplete variables were imputed under fully conditional specification, using the default settings of the mice 3.0 package (Van Buuren and Groothuis-Oudshoorn 2011)*. The parameters of substantive interest were estimated in each imputed dataset separately, and combined using Rubin's rules. For comparison, *we also performed the analysis on the subset of complete cases.*
:::

## Report

-   *Amount of missing data*
-   *Reasons for missingness*
-   *Consequences*
-   *Method*
-   *Imputation model*
-   *Pooling*
-   *Software*
-   *Complete-case analysis*

## Is it MCAR, MAR, NMAR?

> The post-experiment manipulation-check questionnaires for five participants were accidentally thrown away.

. . .

-   MCAR

## Is it MCAR, MAR, NMAR?

> In a 2-day memory experiment, people who know they would do poorly on the memory test are discouraged and don't want to return for the second session

. . .

-   NMAR

## Is it MCAR, MAR, NMAR?

> A health psychologist is surveying high school students on marijuana use. Students who scored highly on anxiety left these questions blank.

. . .

-   MAR

## Wrap up

-   When you have missing data, think about WHY they are missing

-   Missing data handled improperly can bias your expectations

-   MI and ML are good ways to handle missing data!

    -   Bayesian methods are good too :)

    -   Inverse probability weighting seem to work well [@gomila2022]
